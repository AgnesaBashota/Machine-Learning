{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# KK2 – Retrieval‑Augmented QA (English)\n", "\n", "This notebook implements a compact RAG pipeline without external API keys.\n", "Pipeline: load → chunk → vectorize (TF‑IDF / BM25 / optional SBERT) → retrieve → answer → evaluate.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Imports & setup\n", "import os, re, json, math, string\n", "from pathlib import Path\n", "from typing import List, Tuple, Dict\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "from rank_bm25 import BM25Okapi\n", "\n", "# Optional semantic model\n", "USE_SBERT = False\n", "SBERT = None\n", "try:\n", "    from sentence_transformers import SentenceTransformer\n", "    SBERT = SentenceTransformer('all-MiniLM-L6-v2')\n", "    USE_SBERT = True\n", "except Exception:\n", "    USE_SBERT = False\n", "print('SBERT enabled:', USE_SBERT)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Load data & make chunks"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["DATA_PATH = Path('data/handbook_en.txt')\n", "raw_text = DATA_PATH.read_text(encoding='utf-8')\n", "\n", "def chunk_text(text: str, chunk_size: int = 120, overlap: int = 20) -> List[str]:\n", "    words = text.split()\n", "    chunks, i = [], 0\n", "    while i < len(words):\n", "        chunk = words[i:i+chunk_size]\n", "        chunks.append(' '.join(chunk))\n", "        i += max(1, chunk_size - overlap)\n", "    return chunks\n", "\n", "chunks = chunk_text(raw_text, chunk_size=120, overlap=20)\n", "pd.DataFrame({'chunk_id': range(len(chunks)), 'text': chunks}).head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Vectorization: TF‑IDF, BM25, (optional) SBERT"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# TF‑IDF\n", "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n", "X_tfidf = tfidf.fit_transform(chunks)\n", "\n", "# BM25\n", "tokenized = [c.split() for c in chunks]\n", "bm25 = BM25Okapi(tokenized)\n", "\n", "# SBERT (if available)\n", "if USE_SBERT:\n", "    X_sbert = SBERT.encode(chunks, normalize_embeddings=True)\n", "else:\n", "    X_sbert = None\n", "print('TF‑IDF:', X_tfidf.shape, '| BM25 docs:', len(tokenized), '| SBERT:', None if X_sbert is None else X_sbert.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Retrieval functions"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from sklearn.preprocessing import normalize\n", "def retrieve(query: str, top_k: int = 3, w_tfidf: float = 1.0, w_bm25: float = 1.0, use_sbert: bool = USE_SBERT, w_sbert: float = 1.0):\n", "    q = query.strip()\n", "    scores = np.zeros(len(chunks), dtype=float)\n", "    # TF‑IDF\n", "    q_tfidf = tfidf.transform([q])\n", "    scores += w_tfidf * cosine_similarity(q_tfidf, X_tfidf)[0]\n", "    # BM25\n", "    bm25_scores = np.array(bm25.get_scores(q.split()))\n", "    # normalize BM25 to 0..1\n", "    if bm25_scores.max() > 0:\n", "        bm25_scores = bm25_scores / bm25_scores.max()\n", "    scores += w_bm25 * bm25_scores\n", "    # SBERT\n", "    if use_sbert and X_sbert is not None:\n", "        q_s = SBERT.encode([q], normalize_embeddings=True)\n", "        sbert_scores = X_sbert @ q_s[0]\n", "        scores += w_sbert * sbert_scores\n", "    # Sort\n", "    idx = np.argsort(-scores)[:top_k]\n", "    return [(int(i), float(scores[i]), chunks[int(i)]) for i in idx]\n", "\n", "retrieve('Where should IT incidents be reported?')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Simple answer generation from retrieved context"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def generate_answer(query: str, top_k=3, min_score=0.05, **weights):\n", "    hits = retrieve(query, top_k=top_k, **weights)\n", "    kept = [(i,s,c) for (i,s,c) in hits if s >= min_score]\n", "    if not kept:\n", "        return {'query': query, 'answer': \"I couldn't find a direct answer in the material.\", 'sources': []}\n", "    context = '\\n'.join([c for (_,_,c) in kept])\n", "    answer = f\"Based on the handbook:\\n{context}\"\n", "    return {'query': query, 'answer': answer, 'sources': [int(i) for (i,_,_) in kept]}\n", "\n", "generate_answer('When is a medical certificate required for sick leave?')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Ask questions"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["questions = [\n", "    'Where should IT incidents be reported?',\n", "    'When is a medical certificate required for sick leave?',\n", "    'What is the mileage reimbursement?'\n", "]\n", "for q in questions:\n", "    res = generate_answer(q, top_k=3)\n", "    print('\\nQ:', q)\n", "    print(res['answer'])\n", "    print('Sources chunk ids:', res['sources'])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Evaluation: EM and token‑level F1"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from collections import Counter\n", "\n", "def normalize_text(s: str) -> str:\n", "    s = s.lower()\n", "    s = ''.join(ch for ch in s if ch.isalnum() or ch.isspace())\n", "    return ' '.join(s.split())\n", "\n", "def f1_score(pred: str, gold: str) -> float:\n", "    p_tokens = normalize_text(pred).split()\n", "    g_tokens = normalize_text(gold).split()\n", "    if not p_tokens and not g_tokens:\n", "        return 1.0\n", "    common = Counter(p_tokens) & Counter(g_tokens)\n", "    num_same = sum(common.values())\n", "    if num_same == 0:\n", "        return 0.0\n", "    precision = num_same / len(p_tokens)\n", "    recall = num_same / len(g_tokens)\n", "    return 2 * precision * recall / (precision + recall)\n", "\n", "def exact_match(pred: str, gold: str) -> int:\n", "    return int(normalize_text(pred) == normalize_text(gold))\n", "\n", "def evaluate(eval_path='eval/eval_set.jsonl'):\n", "    ems, f1s = [], []\n", "    with open(eval_path, 'r', encoding='utf-8') as f:\n", "        for line in f:\n", "            ex = json.loads(line)\n", "            q, gold = ex['question'], ex['answer']\n", "            ans = generate_answer(q)['answer']\n", "            # crude heuristic: take last line as the most specific sentence\n", "            # or just use the whole answer\n", "            pred = ans\n", "            ems.append(exact_match(pred, gold))\n", "            f1s.append(f1_score(pred, gold))\n", "    return {'EM': float(np.mean(ems)), 'F1': float(np.mean(f1s))}\n", "\n", "evaluate()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}